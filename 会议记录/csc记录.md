我本周看斯坦福网课的笔记，主要看了3节课的内容
# 神经网络
   主要学了神经网络的工作方式，简要了解了其中的一些内容
## 交叉熵
为了评价该预测的结果好坏，我们会计算其交叉熵(Cross entropy)：
H(p,q)=∑xp(c)log1/q(c)
q(c)为算法预测样本的标记y是c的概率，取值在0-1之间。p(c)为样本的标记y客观上是否真的为c，取值为0（样本的标记不是c）或1（样本的标记取值是c）。
## 迁移算法
迁移学习顾名思义就是把已经训练好的模型的参数迁移到新的模型来帮助新模型的训练。考虑到大部分的数据或任务是存在相关性的，
所以通过迁移学习我们可以将已经学习到的模型参数，通过某种方式来分享给新的模型，从而加快优化模型的学习效率不用像大多数模型那样从零开始。
### fine-tune
fine-tune是进行迁移学习的一种手段。一般我们本身的数据集量比较小，无法重头开始训练一个效果良好的模型。
于是，我们只能通过迁移学习，将一个网络，比如（VGG）前面5大层保持模型参数不变（这里的模型参数是指已经通过Imagenet数据集训练好的模型参数）。
因为前面几层主要是提取图片的特征。因此，我们也可以把这几层当作特征提取器，保持原有的权重不变，提取现有的图片的特征。
微调就是将调整后面的全连接层，或者最后的几个卷积层加全连接层，来实现我们想要的结果，一般根据自己要分类的类别数等等进行调整。
### fine-tune使用场景
a.新数据集比较小且和原数据集相似，因为新数据集比较小，如果fine-tune可能会过拟合;又因为新旧数据集类似，我们期望他们高层特征类似，可以使用预训练网络当作特征提取器，用提取的特征训练线性分类器。
b. 新数据集比较大且和原数据集类似，因为新数据集足够大，可以微调整个网络。
c. 新数据集比较小且和原数据集不相似，新数据集比较小，最好不要fine-tune。和原数据集不类似，最好也不要使用高层特征。这时，可以使用前面的特征来训练分类器。
d.新数据集大且和原数据集不相似。因为新数据集足够大，可以重新训练，但是在实践中fine-tune预训练的模型还是有益的，新数据集足够大，可以微调整个网络。
## 激活函数
tanh在NLP中很常用，尤其是在LSTM和GRU中。不过通过计算可以发现，tanh其本质就是2*sigmoid-1 。
hard tanh激活函数是tanh的变体，当x的绝对值大于1时，函数的梯度为0；当x的绝对值小于1时，函数的梯度为1。这样便捷了反向传播时的计算。
ReLU是如今最受欢迎的激活函数，y = max(0, x)。其和hard tanh比较相似：当神经元上的数值小于0时，激活函数的梯度为0，不更新该神经元对应的参数W和b；而当神经元上的参数大于0时，激活函数的梯度为1，更新该神经元对应的参数W和b。
使用ReLU，X小于零时，对应的参数不会被更新。有人认为，我们也可以更新那些X小于零的神经元对应的参数，不过更新的梯度要小一点，比如x>0梯度为1，x<0梯度为0.1（如果x<0时梯度也为1，那就和不使用激活函数一样了，最后得到的神经网络就是一个线性分类器）
于是，就有了LeakyReLU：y = max(0, x) + leak*min(0,x) 。
## 命名实体识别

# 反向传播
我觉得这本质上就要是一种链式的梯度下降

# RNN和与语言模型
基本理解了RNN模型，还理解了损失函数
